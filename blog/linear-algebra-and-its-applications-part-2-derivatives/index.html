<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="text/html; charset=UTF-8" http-equiv=content-type><meta content="width=device-width,initial-scale=1,user-scalable=no" name=viewport><meta content="index, follow" name=robots><link href=icons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=icons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=icons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=icons/site.webmanifest rel=manifest><script data-website-id=fa80f253-25df-4683-b755-8a0caa0b0151 defer src=https://cloud.umami.is/script.js></script><title>Linear Algebra and Its Applications, Part 2: Derivatives</title><meta content="Linear Algebra and Its Applications, Part 2: Derivatives" name=title><meta content="Simon Xiang" name=author><meta content="Simon Xiang's personal website." name=description><meta content=website property=og:type><meta content=https://simonxiang.xyz/blog/linear-algebra-and-its-applications-part-2-derivatives/ property=og:url><meta content="Home | Simon Xiang" property=og:site_name><meta content="Linear Algebra and Its Applications, Part 2: Derivatives" property=og:title><meta content="Simon Xiang's personal website." property=og:description><meta content=https://simonxiang.xyz/favicon.ico property=og:image><link href=https://simonxiang.xyz/blog/linear-algebra-and-its-applications-part-2-derivatives/ rel=canonical><link rel="shortcut icon" href=https://simonxiang.xyz/favicon.ico type=image/x-icon><link href=https://simonxiang.xyz/atom.xml rel=alternate title=RSS type=application/atom+xml><link href=https://simonxiang.xyz/css/reset-min.css rel=stylesheet><link href=https://simonxiang.xyz/css/sucss-min.css rel=stylesheet><link href=https://simonxiang.xyz/css/style.css rel=stylesheet><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib rel=stylesheet><script crossorigin defer integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js></script><script crossorigin defer integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError : false
        });
    });</script><script defer src=https://simonxiang.xyz/js/script.js></script><body><header><nav id=nav-bar><a href=/> Home </a>  <a href=/about> About </a>  <a href=/resume> CV </a>  <a href=/blog> Blog </a>  <div><input id=theme-toggle style=display:none type=checkbox><label for=theme-toggle id=theme-toggle-label><svg class=icons id=theme-icon><use href=https://simonxiang.xyz/icons.svg#lightMode></use></svg></label></div></nav></header><main><time datetime=2020-08-05>Published on: <span class=accent-data>2020-08-05</span></time><address rel=author>By <span class=accent-data>Simon Xiang</span></address><h1><em>[Archive]</em> Linear Algebra and Its Applications, Part 2: Derivatives</h1><p>I mentioned in my introduction post that this series would probably end up being about the applications of Linear Algebra to other fields of math or something. Well it’s the first post, and we already stopped talking about real life applications! Whoops.<p>Consider the vector space with basis $$ \mathscr{B} = \{ 1, x, x^2, … , x^n \} $$ over $\mathbb{R}.$ This is known as the <strong>vector space of polynomials with real coefficients of degree n or less</strong>, denoted by $\mathbb{R}[x]$ (some texts may use $P_n)$. If it looks familiar, it probably showed up as a frequent example in your Linear Algebra problem sets (and is of extreme importance over an arbitrary field in Galois Theory)!<p>Note: while in Algebra the PID $\mathbb{F}[x]$ for $\mathbb{F}$ a field contains infinite degree polynomials, in this case we will assume $\mathbb{R}[x]$ to have finite degree polynomials with maximum degree $n$.<p>We can write an arbitrary element of any vector space as a <strong>linear combination</strong> of the elements of its basis set. In this case, an element of $\mathbb{R}[x]$ looks like a polynomial. A linear combination of the elements of $\mathscr{B}$ is of the form $$ a + a_1x + a_2x^2 + … a_nx^n. $$ We can use $f(x)$ and $g(x)$ to denote such linear combinations with shorthand $\sum_{i=0}^{n} a_ix^i$ for $a_i \in \mathbb{R}$. Note that $x$ <em>isn’t actually a variable:</em> we haven’t defined a way to evaluate $x$ and it doesn’t change based off the input. (If you’re wondering how we evaluate polynomials in the traditional sense, we use something called the <em>evaluation homomorphism</em>).<p>We can use this vector space to do some cool things, like take ideas from Calculus and express them in the language of Linear Algebra! Remember the <strong>derivative</strong> from Calculus: in this case it’s a map $\frac{d}{dx} : \mathbb{R}[x] \to \mathbb{R}[x]$ such that $$\frac{d}{dx}\left(a+a_1x+…+a_nx^n\right)=a_1+2a_2x+…+na_nx^{n-1}.$$ In summation notation, it’s saying that $$\frac{d}{dx}\sum_{i=0}^{n} a_ix^i = \sum_{i=0}^{n-1} (i+1)a_{i+1}x^{i}.$$ Using the standard notations for derivatives, we can write $\frac{d}{dx}f(x)=f’(x)$ for $f(x)\in\mathbb{R}[x].$<p>We can show that the map $\frac{d}{dx} : \mathbb{R}[x] \to \mathbb{R}[x]$ is <strong>linear</strong>. Recall that the conditions for a map $T: V \to V$ to be linear are that $$ \text{1:},,T(v_1+v_2)=T(v_1)+T(v_2) $$ $$ \text{2:},,T(\alpha v) = \alpha T(v) $$ for a vector space $V$ over a field $\mathbb{F}, v_i \in V, \alpha \in \mathbb{F}.$ We know that $$ \frac{d}{dx}\left(f(x) + g(x)\right) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x) $$ for $f(x), g(x) \in \mathbb{R}[x],$ satisfying the first condition. Next, $$ \frac{d}{dx}\left( c f(x) \right) = c \frac{d}{dx}f(x) $$ for $c \in \mathbb{R}, f(x) \in \mathbb{R}[x],$ so $\frac{d}{dx}$ is linear.<p>Furthermore, if a map from a vector space onto itself is linear, we can say it’s a <em><strong>linear transformation.</strong></em> We can represent such transformations with a <strong>matrix.</strong> To find the matrix representation of a linear transformation, examine the column vectors that arise from the <strong>image of the basis set under such linear transformation.</strong> We denote this matrix as $[T]_B$ for a vector space $T$ with basis set $B$, so for the derivative operator we would denote the image of $\mathscr{B}$ under $\frac{d}{dx}$ as $[\frac{d}{dx}]_{\mathscr{B}}$.<p>To find the first column vector, examine the image of $1$ under $\frac{d}{dx}$: clearly it vanishes since constants don’t change. So $ [1]_{\frac{d}{dx}}= \Bigg[\begin{smallmatrix} 0 \ \vdots \ 0 \ \end{smallmatrix}\Bigg] $ Similarly, we have $$ [x]_{\frac{d}{dx}} = \begin{bmatrix} 1 \ 0 \ \vdots \ 0 \ \end{bmatrix}, ,[x^2]_{\frac{d}{dx}} = \begin{bmatrix} 0 \ 2 \ 0 \ \vdots \ 0 \ \end{bmatrix}, ,[x^3]_{\frac{d}{dx}} = \begin{bmatrix} 0 \ 0 \ 3 \ 0 \ \vdots \ 0 \ \end{bmatrix}, $$ and so on. Intuitively, this is because $\frac{d}{dx}x=1, \frac{d}{dx}x^2 = 2x, \frac{d}{dx}x^3=3x^2,$ etc, and we represent those values with the vectors above if we write them as a linear transformation of the elements of $\mathscr{B}$ (e.g. $2x=0\cdot 1+2x+0x^2 + \cdots$, $3x^2=0\cdot 1+0x+3x^2+0x^3+ \cdots).$<p>If we combine all these column vectors, we can find a matrix representation of the derivative! Here it is in all its glory: $$ [\frac{d}{dx}]_{\mathscr{B}} = \begin{pmatrix} 0 & 1 & 0 & 0 & \cdots & 0 \ 0 & 0 & 2 & 0 & \cdots & 0 \ 0 & 0 & 0 & 3 & \cdots & 0 \ 0 & 0 & 0 & 0 & \ddots & \vdots \ \vdots & \vdots & \vdots & \vdots & \ddots & n \ 0 & 0 & 0 & 0 & 0 & 0 \ \end{pmatrix}, $$ where $\dim (\mathbb{R}[x]) = n+1$ (or $\mathbb{R}[x]$ having polynomials of a maximum degree $n$). You can multiply this matrix by some polynomial vectors in your free time to see if this really works.<p>Furthermore, the derivative matrix has an interesting algebraic property in that it’s <strong>nilpotent.</strong> A nilpotent matrix is defined as one that eventually vanishes when multiplied by itself, that is, there exists some $k \in \mathbb{Z}$ such that $$ A^k = 0, $$ where $A$ is a matrix and $0$ denotes the zero matrix.<p>I won’t offer a proof, but this is an intuitive result: recall from Analysis that no polynomial of finite degree is infinitely differentiable. So an arbitrary polynomial of degree $n$ will vanish if differentiated $n+1$ times, or in other words, multiplied by the matrix $\frac{d}{dx}^{n+1}$ (in Liebniz notation this would be denoted as $\frac{d^{n+1}}{dx^{n+1}}$). So clearly $\frac{d^{n+1}}{dx^{n+1}}$ corresponds to the zero matrix, and $k=n+1.$<p class=tags-data></main><footer><hr><div id=footer-container><div><p>Built with <a rel="noopener noreferrer" href=https://www.getzola.org target=_blank>Zola</a> using the <a rel="noopener noreferrer" href=https://github.com/Speyll/anemone target=_blank>anemone</a> theme.</div><div><a rel="noopener noreferrer" title="Subscribe via RSS for updates." class=no-style href=https://simonxiang.xyz/atom.xml target=_blank><svg class=icons><use href=https://simonxiang.xyz/icons.svg#rss></use></svg></a></div></div></footer>